{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39579f0",
   "metadata": {},
   "source": [
    "### Spark 데이터 처리 프레임워크\n",
    "\n",
    "|     |비 실시간 처리|실시간 처리|\n",
    "|---|---|---|\n",
    "|정형 데이터|Dataframe|Structured Streaming|\n",
    "|비정형 데이터|RDD|Dstream|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62abd66d",
   "metadata": {},
   "source": [
    "### 빅데이터의 현실적 정의\n",
    "- 정의\n",
    "    - 현재의 데이터베이스 시스템 아키텍처와 관계형 DBMS를 사용해서는 처리 및 관리가 쉽지 않은 데이터\n",
    "    - 현재의 관계형 데이터베이스 시스템 아키텍처\n",
    "        - 단일 또는 분산 서버가 마치 하나의 서버에 데이터가 저장된 것처럼\n",
    "          (투명성 (transparency)) 저장 관리\n",
    "        - 데이터의 무결성을 유지하면서 갱신 관리 (ACID)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e3b8a",
   "metadata": {},
   "source": [
    "### Stream Processing Overview\n",
    "\n",
    "- Stream data를 아주 작은 RDD로 나누어서, RDD처럼 처리\n",
    "-> DStream(discretized Streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76d6e3",
   "metadata": {},
   "source": [
    "### DStream Sources\n",
    "\n",
    "1. socketTextStream\n",
    "    - TCP/IP 소켓에서 데이터를 바로 수신\n",
    "    - hostname과 port number를 지정\n",
    "    - 테스트용으로 사용\n",
    "    \n",
    "2. textFileStream\n",
    "    - 데이터 입력 폴더에서, 새로이 만들어지는 파일의 데이터를 읽어들임\n",
    "    - 기존에 데이터가 파일로 있을 경우나 새로 만들어지는 데이터들이 파일로 저       장될 경우, 이렇게 생성되는 파일들을 읽어서 스트리밍 데이터를 처리.\n",
    "\n",
    "3. Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63781c2",
   "metadata": {},
   "source": [
    "### socketTextStream을 위한 컴퓨터 환경 구성\n",
    "- 윈도우에서 2개의 터미널을 연결.\n",
    "    - 첫번째 터미널: pyspark 프로그램 실행\n",
    "        - Anaconda의 Jupyter Notebook에서 프로그래밍 작업 수행\n",
    "\n",
    "- 두번째 터미널: 스트리밍 데이터 생성\n",
    "    - 터미널에서 송신 포트를 설정하고, 데이터 송신 준비를 함\n",
    "      (anaconda의 환경설정 (Environment)에서 Open Terminal)을 선택하여 새로운 터미널을 엶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e1482",
   "metadata": {},
   "source": [
    "### socketTextStream을 위한 Spark Streaming Context 구성\n",
    "\n",
    "- import libraries\n",
    "\n",
    "        import pyspark\n",
    "        from pyspark import SparkConf, SparkContext\n",
    "        from pyspark.streaming import StreamingContext\n",
    "\n",
    "- Master 지정\n",
    "    - Localhost에 여러 개의 thread를 실행하여 그중 하나를 master로 지정 이때 서버와 클라이언트 사이의 데이터 통신이 필요하므로, master local(2) 등의 임의의 숫자를 입력하여 local server를 실행\n",
    "    - 그냥 local로 쓰면, 1개의 thread만 실행됨을 의미하며, 따라서 스트리밍 데이터를 방생시키는 thread와 통신이 이루어지지 않음.\n",
    "    \n",
    " sc = pyspark.SparkContext(master=\"local[2]\")\n",
    " \n",
    " - 대기시간을 지정: Seconds()\n",
    " \n",
    " ssc = StreamingContext(sc,5) -> 5초 단위\n",
    " \n",
    " ### SparkStreaming에서 데이터 수신\n",
    " - socketTextStream:\n",
    "     - 통신 socket으로 데이터를 수신, 주로 테스트 용도로 사용\n",
    "     - port number는 nc(ncat)에서 지정한 port number와 동일해야 함\n",
    "         -> 수신 데이터는 DStream (Discretized Stream)으로 변환됨.\n",
    "         \n",
    " lines = ssc.socketTextStream(\"localhost\",9999)\n",
    " \n",
    " ### DStream 데이터 변환 처리\n",
    " - RDD의 변환 함수들과 동일한 변환 함수들을 사용해서 Dstream 데이터를 변환 처리 함\n",
    " \n",
    "         import re\n",
    "         words = lines.flatMap(lambda line: re.split(\"\\W+\", line))\n",
    "\n",
    "         words = words.filter(lambda word: len(word)>0) \\\n",
    "         .map(lambda w: w.lower()) \\\n",
    "         .map(lambda w: (w, 1)) \\\n",
    "         .reduceByKey(lambda v1, v2: v1 + v2) \\\n",
    "         .map(lambda r: (r[1],r[0])) \\\n",
    "         .transform(lambda rdd: rdd.sortByKey()) \\\n",
    "         .map(lambda r: (r[1],r[0]))\n",
    "\n",
    "         words.pprint()\n",
    "         \n",
    "### 스트리밍 작업 시작과 실행\n",
    "- Spark Streaming 데이터 처리 실행\n",
    "    - ssc.start()\n",
    "    - ssc.waitTermination 또는\n",
    "      ssc.waitTerminatingOrTimeout(180) : Timeout 시간이 180 seconds인 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707140a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49fe2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffeb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ed97eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bb24bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b7495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(master=\"local[2]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d35a928",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd670619",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\",9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4af568c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d39d5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: re.split(\"\\W+\", line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa495c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.filter(lambda word: len(word)>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fed61956",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsLower = words.map(lambda w: w.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "094773c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordAndOne = wordsLower.map(lambda w: (w,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f868a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = wordAndOne.reduceByKey(lambda v1, v2: v1 + v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f418eb9",
   "metadata": {},
   "source": [
    "### Tranformations on DStreams\n",
    "\n",
    "- map(),flatMap(),filter(),union(),reduceByKey(),join() ...\n",
    "- sortBy(),sortByKey()는 없음\n",
    "\n",
    "- 해결방법\n",
    "    - transform()을 활용:\n",
    "      Returns a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream\n",
    "    - Example:\n",
    "      .transform(lambda rdd: rdd.sortBy(lambda x: x[1],False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ebddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedWC = wordCount.transform(lambda rdd:rdd.sortBy(lambda x:x[1],False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "355a37dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedWC.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b836806",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9d15ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5f9f81",
   "metadata": {},
   "source": [
    "1. netcat 설치\n",
    "2. 환경변수 설정\n",
    "3. cmd 창에서 nc.exe -lvp 9999 입력할 것!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
