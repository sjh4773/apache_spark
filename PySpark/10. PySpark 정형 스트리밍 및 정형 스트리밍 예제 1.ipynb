{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee1365c",
   "metadata": {},
   "source": [
    "### Streaming Data 처리\n",
    "- Streaming 데이터를 처리할 때 DataFrame을 사용하기 위해서는\n",
    "    - Spark Session(즉 spark 변수 사용)을 사용하여 처리\n",
    "- ssc에서는 DataFrame의 변환이 불가능 즉 .toDF 변화를 실행하면 오류 발생\n",
    "    - RDD로 계속 작업\n",
    "- Dataframe의 데이터 입출력 함수를 제공\n",
    "|     |Read|Write|\n",
    "|---|---|---|\n",
    "|일반 데이터  |spark.read|spark.write|\n",
    "|Stream 데이터|spark.readStream|spark.writeStream|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49285db6",
   "metadata": {},
   "source": [
    "### 예제\n",
    "1. 콘솔 입력 데이터의 단어 발생빈도 측정.\n",
    "2. Dessert menu 주문이 스트리밍 데이터로 입력되면,\n",
    "   Menu별 가격 데이터를 사용하여 주문별 금액을 계산하여 출력\n",
    "3. Menu별로 시간대별로 주문 현황이 어떻는가를 집계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ac5dd2",
   "metadata": {},
   "source": [
    "### DataFrame Read\n",
    "- File source\n",
    "    - Format 사용\n",
    "        - csv, json 등의 형식으로 데이터를 읽을 수 있음\n",
    "- Socket source\n",
    "    - text string으로 읽음.\n",
    "        - 1개의 컬럼으로 구성된 DataFrame이 만들어짐.\n",
    "        - 여러 개의 컬럼으로 구성된 Schema를 사용할 수 없음.\n",
    "    - 테스트용으로만 사용 권장\n",
    "        - End-to-end fault tolerance를 제공하지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58867ab8",
   "metadata": {},
   "source": [
    "### SturcturedStreaming Input Source\n",
    "- socket source\n",
    "    - 터미널 1:\n",
    "        - spark.readStream.format(\"socket\").option(\"host\",\"localhost\").option(\"port\",9999).load()\n",
    "    - 터미널 2:\n",
    "        - nc -lvp 9999\n",
    "        \n",
    "- Files in a directory\n",
    "    - spark.readStream.schema(mySchema).format(\"csv\").load(\"/my/path/input\")\n",
    "    \n",
    "- Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7547ae7",
   "metadata": {},
   "source": [
    "### StructuredStreaming Output Sink\n",
    "- Console\n",
    "    - Queryname.writeStream.format(\"console\").option(\"checkpointLocation\",\"/path/to/directory\").outputMode(\"append\").start()\n",
    "- Memory\n",
    "    - Queryname.writeStream.queryName(\"table name\").format(\"memory\").outputMode(\"complete\").start()\n",
    "- File\n",
    "    - Queryname.writeStream.format(\"csv\").option(\"path\",\"/path/to/directory\").option(\"checkpointLocation\",\"/path/to/directory\").outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd584cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360f04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7e6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4caaba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc5bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec26a79",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c00ec66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession  \\\n",
    ".builder \\\n",
    ".master('local[2]') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68dce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "linesDF = spark.readStream \\\n",
    ".format(\"socket\") \\\n",
    ".option(\"host\", \"localhost\") \\\n",
    ".option(\"port\", 9999) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82d7b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95ff83f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(linesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ce47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = linesDF.select(explode(split(linesDF.value,\" \")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eda659b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5c9355",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = words.groupby(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "228a7cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCount.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3794d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4eb56a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "Query = wordCount.writeStream.format(\"console\")  \\\n",
    ".outputMode(\"complete\")  \\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f08d0e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.streaming.StreamingQuery"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
