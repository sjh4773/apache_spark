{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f68467b",
   "metadata": {},
   "source": [
    "### Spark의 데이터 처리 아키텍처\n",
    "1. 데이터 처리 : Dataset(RDD 또는 Dataframe)의 변환 과정\n",
    "- 데이터를 변경하는게 아니라, 새로운 dataset을 생성\n",
    "\n",
    "2. Spark은 함수형 프로그래밍 언어\n",
    "- 함수(변환, transformation)의 인자로 함수를 받아들일 수 있음.\n",
    "        Dataset2 = Dataset1.변환함수1\n",
    "        Dataset3 = Dataset2.변환함수2 -> Dataset1.변환함수1.변환함수2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f4d67",
   "metadata": {},
   "source": [
    "### RDD의 종류\n",
    "- (base) RDD\n",
    "    - Element(레코드라고도 할 수 있음)들로 구성됨\n",
    "    - Element\n",
    "        - int, Double, String 등의 데이터 뿐만 아니라\n",
    "          Tuple 일 수도 있고, Array 일수도 있음.\n",
    "          (string, csv 등을 split 하면 Array가 만들어짐)\n",
    "\n",
    "- Pair RDD\n",
    "    - (key, value) -> 위치에 의하여 정해진다\n",
    "- Multi-column RDD\n",
    "    - ([0], [1], [2], ...)\n",
    "- DataFrame\n",
    "    - (colname1, colname2, ..., colnameN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62466cbd",
   "metadata": {},
   "source": [
    "### RDD 생성\n",
    "\n",
    "- parallelizing an exisiting collection in your driver program,\n",
    "> numRDD = sc.parallelize([1,2,3,4,5])\n",
    "\n",
    "- referencing a dataset in an external storage system\n",
    "    - a shared file system\n",
    "    - HDFS\n",
    "    - HBase\n",
    "    - any data source offering a Hadoop input Format\n",
    "> lines = sc.textFile(\"C://spark/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e307f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76926e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8095e523",
   "metadata": {},
   "outputs": [],
   "source": [
    "numRDD = sc.parallelize([1, 2, 3, 4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c863c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "043e82d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(numRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77951e",
   "metadata": {},
   "source": [
    "3. 지연 실행 (lazy evaluation)\n",
    "- 데이터 변환 함수(transformation)를 곧바로 실행하지 않음\n",
    "    - 변형 방법만 기록해 둠.\n",
    "    - 데이터 출력이나 저장 활동이 일어날 때 기억해 둔 모든 변환 작업을 한꺼번에 실행함.\n",
    "- 프로그램 개발 과정에서는 각 변환 함수를 실행할 때마다, 출력 등의 활동(Action)을\n",
    "  실행해서 입력된 변환 명령어가 오류가 없는지를 확인해보는 것이 좋음.\n",
    "     - 완성된 프로그램은 함수형으로 표현하는 것이 간결함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c283ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"C://spark/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58ded6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "words = lines.flatMap(lambda line: re.split('\\\\W+', line)) \\\n",
    "             .filter(lambda word: len(word) > 0) \\\n",
    "             .map(lambda word: word.lower()) \\\n",
    "             .map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec5e85ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0ebfe",
   "metadata": {},
   "source": [
    "### Spark transformations : Basic Operations\n",
    "\n",
    "\n",
    "#### 개별 원소들을 1:1로 매핑시킨다.\n",
    "- map(func)\n",
    "    - 데이터셋의 각 원소들에게 func를 적용한 새로운 데이터셋을 생성\n",
    "    > wordAndOne = words.map(lambda x: (x,1))\n",
    "\n",
    "#### 함수\n",
    "- def 함수명(인자명: 인자타입) -> 결과타입\n",
    "    - Python은 변수의 유형(type)을 지정하지 않아도 되며, 지정해도 확인하지 않음\n",
    "    - Default 값을 인자에 부여할 수 있음\n",
    "\n",
    "\n",
    "#### Spark에서 사용자가 정의한 함수의 사용\n",
    "- 변환함수의 인자로 사용자정의함수를 사용하면, Dataset의 모든 원소에 대하여\n",
    "  사용자 정의함수를 적용하는 변환이 이루어짐.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a560a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySquare(x: float) -> float:\n",
    "    return x * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed67f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "numSquare = numRDD.map(mySquare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da68d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "for e in numSquare.collect(): # collect() : RDD를 파이썬의 list로 바꿔주는 함수\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c8d0c",
   "metadata": {},
   "source": [
    "#### 익명 함수\n",
    "- 일반적인 함수: 실명함수\n",
    "    - 이름을 부여하고\n",
    "    - 입력 변수를 지정하고\n",
    "    - 결과값을 돌려받음\n",
    "- 익명 함수\n",
    "    - 함수의 이름을 부여하지 않고,\n",
    "    - 입력 변수와\n",
    "    - 함수식만을 정의해서 결과값을 돌려받음\n",
    "- 표시방법\n",
    "    - Python\n",
    "        - lambda\n",
    "    - Scala\n",
    "        - =>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1cbb04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numSquare2 = numRDD.map(lambda x: x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dedda675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numSquare2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb383f",
   "metadata": {},
   "source": [
    "- flatMap(func)\n",
    "    - map과 비슷하나, 데이터셋의 각 원소들을 여러 개의 원소로 매핑함.\n",
    "      (따라서 func은 한개의 원소가 아니라 여러 원소들의 연속체를 만들어 냄.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5256a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcefd00",
   "metadata": {},
   "source": [
    "- filter(func)\n",
    "    - func을 실행한 값이 참(True)인 원소들만을 선택하여 새로운 데이터셋을 생성\n",
    "    - returns true.\n",
    "    > words = words.filter(lambda w: len(w) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176a78c",
   "metadata": {},
   "source": [
    "### RDD Actions\n",
    "\n",
    "- collect()\n",
    "    - 모든 원소들의 파이썬 리스트를 생성\n",
    "- count()\n",
    "    - 원소들의 개수를 출력\n",
    "- first()\n",
    "    - 첫번째 원소를 출력\n",
    "- take(n)\n",
    "    - 처음 n개의 원소들을 포함하는 리스트를 생성\n",
    "- top(n)\n",
    "    - Return the top n elements from an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eacbcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45752a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['will run the Pi example locally.',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'storage systems. Because the protocols have changed in different versions of']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266c4b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Structured Streaming for stream processing.',\n",
       " '',\n",
       " '<https://spark.apache.org/>',\n",
       " '',\n",
       " '[![Jenkins Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7-hive-2.3)',\n",
       " '[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       " '[![PySpark Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)',\n",
       " '',\n",
       " '',\n",
       " '## Online Documentation',\n",
       " '',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " 'guide, on the [project web page](https://spark.apache.org/documentation.html).',\n",
       " 'This README file only contains basic setup instructions.',\n",
       " '',\n",
       " '## Building Spark',\n",
       " '',\n",
       " 'Spark is built using [Apache Maven](https://maven.apache.org/).',\n",
       " 'To build Spark and its example programs, run:',\n",
       " '',\n",
       " '    ./build/mvn -DskipTests clean package',\n",
       " '',\n",
       " '(You do not need to do this if you downloaded a pre-built package.)',\n",
       " '',\n",
       " 'More detailed documentation is available from the project site, at',\n",
       " '[\"Building Spark\"](https://spark.apache.org/docs/latest/building-spark.html).',\n",
       " '',\n",
       " 'For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](https://spark.apache.org/developer-tools.html).',\n",
       " '',\n",
       " '## Interactive Scala Shell',\n",
       " '',\n",
       " 'The easiest way to start using Spark is through the Scala shell:',\n",
       " '',\n",
       " '    ./bin/spark-shell',\n",
       " '',\n",
       " 'Try the following command, which should return 1,000,000,000:',\n",
       " '',\n",
       " '    scala> spark.range(1000 * 1000 * 1000).count()',\n",
       " '',\n",
       " '## Interactive Python Shell',\n",
       " '',\n",
       " 'Alternatively, if you prefer Python, you can use the Python shell:',\n",
       " '',\n",
       " '    ./bin/pyspark',\n",
       " '',\n",
       " 'And run the following command, which should also return 1,000,000,000:',\n",
       " '',\n",
       " '    >>> spark.range(1000 * 1000 * 1000).count()',\n",
       " '',\n",
       " '## Example Programs',\n",
       " '',\n",
       " 'Spark also comes with several sample programs in the `examples` directory.',\n",
       " 'To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       " '',\n",
       " '    ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'will run the Pi example locally.',\n",
       " '',\n",
       " 'You can set the MASTER environment variable when running examples to submit',\n",
       " 'examples to a cluster. This can be a mesos:// or spark:// URL,',\n",
       " '\"yarn\" to run on YARN, and \"local\" to run',\n",
       " 'locally with one thread, or \"local[N]\" to run locally with N threads. You',\n",
       " 'can also use an abbreviated class name if the class is in the `examples`',\n",
       " 'package. For instance:',\n",
       " '',\n",
       " '    MASTER=spark://host:7077 ./bin/run-example SparkPi',\n",
       " '',\n",
       " 'Many of the example programs print usage help if no params are given.',\n",
       " '',\n",
       " '## Running Tests',\n",
       " '',\n",
       " 'Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       " 'can be run using:',\n",
       " '',\n",
       " '    ./dev/run-tests',\n",
       " '',\n",
       " 'Please see the guidance on how to',\n",
       " '[run tests for a module, or individual tests](https://spark.apache.org/developer-tools.html#individual-tests).',\n",
       " '',\n",
       " 'There is also a Kubernetes integration test, see resource-managers/kubernetes/integration-tests/README.md',\n",
       " '',\n",
       " '## A Note About Hadoop Versions',\n",
       " '',\n",
       " 'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       " 'storage systems. Because the protocols have changed in different versions of',\n",
       " 'Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       " '',\n",
       " 'Please refer to the build documentation at',\n",
       " '[\"Specifying the Hadoop Version and Enabling YARN\"](https://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version-and-enabling-yarn)',\n",
       " 'for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       " 'building for particular Hive and Hive Thriftserver distributions.',\n",
       " '',\n",
       " '## Configuration',\n",
       " '',\n",
       " 'Please refer to the [Configuration Guide](https://spark.apache.org/docs/latest/configuration.html)',\n",
       " 'in the online documentation for an overview on how to configure Spark.',\n",
       " '',\n",
       " '## Contributing',\n",
       " '',\n",
       " 'Please review the [Contribution to Spark guide](https://spark.apache.org/contributing.html)',\n",
       " 'for information on how to get started contributing to the project.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f51432dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = lines.flatMap(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "513bd2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Apache',\n",
       " 'Spark',\n",
       " '',\n",
       " 'Spark',\n",
       " 'is',\n",
       " 'a',\n",
       " 'unified',\n",
       " 'analytics',\n",
       " 'engine']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e78000d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7498bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: re.split(\"\\W+\", line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8b117c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Apache', 'Spark']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d5e784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.filter(lambda word: len(word) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c6edfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache', 'Spark', 'Spark']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b43b3860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addOne(x):\n",
    "    return (x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9959d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordAndOne = words.map(addOne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98788eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 1), ('Spark', 1), ('Spark', 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordAndOne.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bdfd581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(wordAndOne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8af52",
   "metadata": {},
   "source": [
    "- reduceByKey(func, [numPartitions])\n",
    "    - (K, V) 쌍의 데이터셋에서 키값이 같은 원소들에 대해서 각 원소의 값에 func를 실행한 데이터를 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a8857e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = wordAndOne.reduceByKey(lambda v1, v2: v1 + v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a9d0c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 2), ('Spark', 17), ('is', 7)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCount.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f645999",
   "metadata": {},
   "source": [
    "- sortByKey(ascending=True,[numPartitions])\n",
    "    - (K, V)쌍의 데이터셋의 원소들을 키값의 순서에 따라 정렬한 데이터셋을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4afbcef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedWC = wordCount.sortByKey(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d05e3ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('your', 1), ('you', 4), ('yarn', 2)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedWC.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44748772",
   "metadata": {},
   "source": [
    "- sortBy(keyfunc,[ascending],[numPartitions])\n",
    "    - (K, V) 쌍의 데이터셋의 원소들을 keyfunc 함수의 순서에 따라 정렬한 데이터셋을 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "680e6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedWC2 = wordCount.sortBy(lambda x: x[1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bc7c6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 24),\n",
       " ('spark', 21),\n",
       " ('Spark', 17),\n",
       " ('https', 16),\n",
       " ('to', 16),\n",
       " ('run', 13),\n",
       " ('for', 12),\n",
       " ('and', 10),\n",
       " ('a', 9),\n",
       " ('apache', 9),\n",
       " ('org', 9),\n",
       " ('is', 7),\n",
       " ('on', 7),\n",
       " ('html', 7),\n",
       " ('example', 7),\n",
       " ('can', 6),\n",
       " ('building', 6),\n",
       " ('000', 6),\n",
       " ('1000', 6),\n",
       " ('tests', 6)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedWC2.take(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
