{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcfefe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596fc99a",
   "metadata": {},
   "source": [
    "### PySpark 실행을 위한 변수 생성\n",
    "- 스파크 실행 변수\n",
    "1. sc(SparkContext 변수) -> RDD\n",
    "2. spark(SparkSession 변수) -> DataFrame \n",
    "\n",
    "- 터미널에서 spark를 실행하면\n",
    "    - spark(SparkSession 변수)가 자동으로 만들어짐\n",
    "    \n",
    "- Jupyter Notebook이나 .py 파일에서 spark 기능을 실행하기 위해서는 직접 만들어주어야 함.\n",
    "    - SparkContext 변수(보통 sc를 사용)\n",
    "    - SparkSession 변수(보통 spark를 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12917406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ea7c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 변수 생성\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d95cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내부/외부 데이터로부터 DataFrame(또는 DataSet) 생성\n",
    "linesDF = spark.read.format(\"text\").load(\"C://spark/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "517992b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='# Apache Spark'),\n",
       " Row(value=''),\n",
       " Row(value='Spark is a unified analytics engine for large-scale data processing. It provides')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesDF.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef90133",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73bad665",
   "metadata": {},
   "outputs": [],
   "source": [
    "linesRDD = sc.textFile('C:/spark/README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f27b860d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d9c62",
   "metadata": {},
   "source": [
    "### SparkContext 생성 방법\n",
    "\n",
    "- SparkConf() 변수를 먼저 생성 후, SparkContext 변수를 생성\n",
    "\n",
    "        from pyspark import SparkContext, SparkConf\n",
    "        conf = pyspark.SparkConf().setMaster('local')\n",
    "        sc = pyspark.SparkContext(conf = conf)\n",
    "\n",
    "- SparkContext() 변수를 직접 생성\n",
    "\n",
    "        from pyspark import SparkContext, SparkConf\n",
    "        sc = pyspark.SparkContext(\"local\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
